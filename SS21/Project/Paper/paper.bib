@inproceedings{hossain-etal-2019-president,
  title = "{``}President Vows to Cut {\textless}Taxes{\textgreater} Hair{''}: Dataset and Analysis of Creative Text Editing for Humorous Headlines",
  author = "Hossain, Nabil  and
      Krumm, John  and
      Gamon, Michael",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
  month = jun,
  year = "2019",
  address = "Minneapolis, Minnesota",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N19-1012",
  doi = "10.18653/v1/N19-1012",
  pages = "133--142",
  abstract = "We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.",
}

@article{DBLP:journals/corr/abs-2008-00304,
  author = {Nabil Hossain and
 John Krumm and
 Michael Gamon and
 Henry A. Kautz},
  title = {SemEval-2020 Task 7: Assessing Humor in Edited News Headlines},
  journal = {CoRR},
  volume = {abs/2008.00304},
  year = {2020},
  url = {https://arxiv.org/abs/2008.00304},
  archivePrefix = {arXiv},
  eprint = {2008.00304},
  timestamp = {Fri, 07 Aug 2020 15:07:21 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-2008-00304.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hossain-etal-2020-funlines,
  title = "{S}timulating Creativity With FunLines: A Case Study of Humor Generation in Headlines",
  author = "Hossain, Nabil  and Krumm, John and Sajed, Tanvir and Kautz, Henry",
  booktitle = "Proceedings of {ACL} 2020, System Demonstrations",
  month = jul,
  year = "2020",
  address = "Seattle, Washington",
  publisher = "Association for Computational Linguistics"
}



@article{DBLP:journals/corr/abs-1907-11692,
  author = {Yinhan Liu and
 Myle Ott and
 Naman Goyal and
 Jingfei Du and
 Mandar Joshi and
 Danqi Chen and
 Omer Levy and
 Mike Lewis and
 Luke Zettlemoyer and
 Veselin Stoyanov},
  title = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal = {CoRR},
  volume = {abs/1907.11692},
  year = {2019},
  url = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author = {Ashish Vaswani and
 Noam Shazeer and
 Niki Parmar and
 Jakob Uszkoreit and
 Llion Jones and
 Aidan N. Gomez and
 Lukasz Kaiser and
 Illia Polosukhin},
  title = {Attention Is All You Need},
  journal = {CoRR},
  volume = {abs/1706.03762},
  year = {2017},
  url = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wolf-etal-2020-transformers,
  title = "Transformers: State-of-the-Art Natural Language Processing",
  author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
  month = oct,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.emnlp-demos.6",
  doi = "10.18653/v1/2020.emnlp-demos.6",
  pages = "38--45",
  abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}


@inproceedings{ammer-gruner-2020-unituebingencl,
  title = "{U}ni{T}uebingen{CL} at {S}em{E}val-2020 Task 7: Humor Detection in News Headlines",
  author = {Ammer, Charlotte and
 Gr{\"u}ner, Lea},
  booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
  month = dec,
  year = "2020",
  address = "Barcelona (online)",
  publisher = "International Committee for Computational Linguistics",
  url = "https://aclanthology.org/2020.semeval-1.139",
  pages = "1060--1065",
  abstract = "This paper describes the work done by the team UniTuebingenCL for the SemEval 2020 Task 7: {``}Assessing the Funniness of Edited News Headlines{''}. We participated in both sub-tasks: sub-task A, given the original and the edited headline, predicting the mean funniness of the edited headline; and sub-task B, given the original headline and two edited versions, predicting which edited version is the funnier of the two. A Ridge Regression model using Elmo and Glove embeddings as well as Truncated Singular Value Decomposition was used as the final model. A long short term memory model recurrent network (LSTM) served as another approach for assessing the funniness of a headline. For the first sub-task, we experimented with the extraction of multiple features to achieve lower Root Mean Squared Error. The lowest Root Mean Squared Error achieved was 0.575 for sub-task A, and the highest Accuracy was 0.618 for sub-task B.",
}


