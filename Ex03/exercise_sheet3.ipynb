{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Statistical Natural Language Processing (WS 20/21)\n",
    "## Exercise Sheet 3 - Manuel Hettich"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function can be used for importing the corpus.\n",
    "Parameters: path_to_file: string; path to the file containing the corpus\n",
    "Returns: list of list; the first layer list contains the sentences of the corpus;\n",
    "    the second layer list contains tuples (token,label) representing a labelled sentence\n",
    "'''\n",
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    f = open(path_to_file)\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "\n",
    "        parts = line.split(' ')\n",
    "        sentence.append((parts[0], parts[-1]))\n",
    "\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MaxEntModel(object):\n",
    "    # training corpus\n",
    "    corpus = None\n",
    "\n",
    "    # (numpy) array containing the parameters of the model\n",
    "    # has to be initialized by the method 'initialize'\n",
    "    theta = None\n",
    "\n",
    "    # dictionary containing all possible features of a corpus and their corresponding index;\n",
    "    # has to be set by the method 'initialize'; hint: use a Python dictionary\n",
    "    feature_indices = None\n",
    "\n",
    "    # set containing a list of possible labels\n",
    "    # has to be set by the method 'initialize'\n",
    "    labels = None\n",
    "\n",
    "    # Caching normalization factors for (word, label)\n",
    "    norm_factors = None\n",
    "\n",
    "\n",
    "    # Exercise 1 a) ###################################################################\n",
    "    def initialize(self, corpus):\n",
    "        \"\"\"\n",
    "        Initialize the maximum entropy model, i.e., build the set of all features, the set of all labels\n",
    "        and create an initial array 'theta' for the parameters of the model.\n",
    "        Parameters: corpus: list of list representing the corpus, returned by the function 'import_corpus'\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # Initialise the set of all words in the corpus\n",
    "        words = set()\n",
    "        # Add a custom label for the beginning of sentences\n",
    "        self.labels = set()\n",
    "        self.labels.add(\"start\")\n",
    "\n",
    "        # Fill the two sets with all the words and labels in the corpus\n",
    "        for sentence in corpus:\n",
    "            for word, label in sentence:\n",
    "                words.add(word)\n",
    "                self.labels.add(label)\n",
    "\n",
    "        # Initialise the feature indices dictionary\n",
    "        self.feature_indices = dict()\n",
    "        # Build the necessary cartesian products and store them enumerated in the dict\n",
    "        product = set()\n",
    "        product.update(itertools.product(self.labels, self.labels))\n",
    "        product.update(itertools.product(words, self.labels))\n",
    "        for idx, feature in enumerate(product):\n",
    "            self.feature_indices.update({feature: idx})\n",
    "\n",
    "        # Initialize theta\n",
    "        self.theta = np.ones(len(self.feature_indices))\n",
    "\n",
    "        # Initialize norm_factors\n",
    "        self.norm_factors = dict()\n",
    "\n",
    "    # Exercise 1 b) ###################################################################\n",
    "    def get_active_features(self, word, label, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the vector of active features.\n",
    "        Parameters: word: string; a word at some position i of a given sentence\n",
    "                    label: string; a label assigned to the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing only zeros and ones.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the indices for the two features\n",
    "        idx_word_label = self.feature_indices.get((word, label), -1)\n",
    "        idx_label_label = self.feature_indices.get((prev_label, label), -1)\n",
    "\n",
    "        # Create a numpy array with the two features set to 1\n",
    "        # according to the indices in self.feature_indices\n",
    "        active_features = np.zeros(len(self.feature_indices), dtype=bool)\n",
    "        if idx_word_label != -1:\n",
    "            active_features[idx_word_label] = True\n",
    "        if idx_label_label != -1:\n",
    "            active_features[idx_label_label] = True\n",
    "\n",
    "        return active_features\n",
    "\n",
    "    # Exercise 2 a) ###################################################################\n",
    "    def cond_normalization_factor(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the normalization factor 1/Z(x_i).\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        \"\"\"\n",
    "\n",
    "        if (word, prev_label) in self.norm_factors:\n",
    "            return self.norm_factors[(word, prev_label)]\n",
    "        else:\n",
    "            z = 0.0\n",
    "            for label in self.labels:\n",
    "                z += np.exp(np.dot(self.theta, self.get_active_features(word, label, prev_label)))\n",
    "\n",
    "            norm_factor = 1 / z\n",
    "            # Cache the normalization factor\n",
    "            self.norm_factors[(word, prev_label)] = norm_factor\n",
    "\n",
    "            return norm_factor\n",
    "\n",
    "\n",
    "    # Exercise 2 b) ###################################################################\n",
    "    def conditional_probability(self, label, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the conditional probability of a label given a word x_i.\n",
    "        Parameters: label: string; we are interested in the conditional probability of this label\n",
    "                    word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        \"\"\"\n",
    "\n",
    "        normalization_factor = self.cond_normalization_factor(word, prev_label)\n",
    "\n",
    "        return normalization_factor * np.exp(np.dot(self.theta, self.get_active_features(word, label, prev_label)))\n",
    "\n",
    "\n",
    "    # Exercise 3 a) ###################################################################\n",
    "    def empirical_feature_count(self, word, label, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the empirical feature count given a word, the actual label of this word and the label of the previous word.\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    label: string; the actual label of the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        \"\"\"\n",
    "\n",
    "        return self.get_active_features(word, label, prev_label)\n",
    "\n",
    "\n",
    "    # Exercise 3 b) ###################################################################\n",
    "    def expected_feature_count(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the expected feature count given a word, the label of the previous word and the parameters of the current model\n",
    "        (see variable theta)\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        \"\"\"\n",
    "\n",
    "        expected_feature_count = np.zeros(len(self.theta))\n",
    "\n",
    "        for label in self.labels:\n",
    "            expected_feature_count += self.get_active_features(word, label, prev_label) * self.conditional_probability(label, word, prev_label)\n",
    "\n",
    "        return expected_feature_count\n",
    "\n",
    "\n",
    "    # Exercise 4 a) ###################################################################\n",
    "    def parameter_update(self, word, label, prev_label, learning_rate):\n",
    "        \"\"\"\n",
    "        Do one learning step.\n",
    "        Parameters: word: string; a randomly selected word x_i at some position i of a given sentence\n",
    "                    label: string; the actual label of the selected word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "                    learning_rate: float\n",
    "        \"\"\"\n",
    "\n",
    "        self.theta += learning_rate * (self.empirical_feature_count(word, label, prev_label) - self.expected_feature_count(word, prev_label))\n",
    "\n",
    "        # Reset the cached normalization factors\n",
    "        self.norm_factors = dict()\n",
    "\n",
    "\n",
    "    # Exercise 4 b) ###################################################################\n",
    "    def train(self, number_iterations, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Implement the training procedure.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    learning_rate: float\n",
    "        \"\"\"\n",
    "\n",
    "        for iteration in range(number_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{number_iterations}\")\n",
    "\n",
    "            # Select a random sentence from the corpus\n",
    "            random_sentence = random.choice(self.corpus)\n",
    "            # Select a random tuple from the random sentence\n",
    "            random_tuple = random.choice(random_sentence)\n",
    "            word = random_tuple[0]\n",
    "            label = random_tuple[1]\n",
    "\n",
    "            # Get the label of the previous tuple in the random sentence\n",
    "            idx_random_tuple = random_sentence.index(random_tuple)\n",
    "            if idx_random_tuple == 0:\n",
    "                prev_label = \"start\"\n",
    "            else:\n",
    "                prev_label = random_sentence[idx_random_tuple - 1][1]\n",
    "\n",
    "            self.parameter_update(word, label, prev_label, learning_rate)\n",
    "\n",
    "\n",
    "    # Exercise 4 c) ###################################################################\n",
    "    def predict(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Predict the most probable label of the word referenced by 'word'\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: string; most probable label\n",
    "        \"\"\"\n",
    "\n",
    "        maximum_probability = -np.inf\n",
    "        result_label = None\n",
    "\n",
    "        # Find the label with the maximum probability\n",
    "        for label in self.labels:\n",
    "            probability = self.conditional_probability(label, word, prev_label)\n",
    "\n",
    "            if probability > maximum_probability:\n",
    "                maximum_probability = probability\n",
    "                result_label = label\n",
    "\n",
    "        return result_label\n",
    "\n",
    "\n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def empirical_feature_count_batch(self, sentences):\n",
    "        \"\"\"\n",
    "        Predict the empirical feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returned by 'import_corpus'\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the empirical feature count array\n",
    "        empirical_feature_count_batch = np.zeros(len(self.theta), dtype=bool)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            first_word = sentence[0][0]\n",
    "            first_label = sentence[0][1]\n",
    "            empirical_feature_count_batch += self.empirical_feature_count(first_word, first_label, \"start\")\n",
    "            for idx, (word, label) in enumerate(sentence[1:]):\n",
    "                prev_label = sentence[idx - 1][1]\n",
    "                empirical_feature_count_batch += self.empirical_feature_count(word, label, prev_label)\n",
    "\n",
    "        return empirical_feature_count_batch\n",
    "\n",
    "\n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def expected_feature_count_batch(self, sentences):\n",
    "        \"\"\"\n",
    "        Predict the expected feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returned by 'import_corpus'\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the expected feature count array\n",
    "        expected_feature_count_batch = np.zeros(len(self.theta))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            first_word = sentence[0][0]\n",
    "            first_label = sentence[0][1]\n",
    "            expected_feature_count_batch += self.expected_feature_count(first_word, first_label)\n",
    "            for idx, (word, label) in enumerate(sentence[1:]):\n",
    "                prev_label = sentence[idx - 1][1]\n",
    "                expected_feature_count_batch += self.expected_feature_count(word, prev_label)\n",
    "\n",
    "        return expected_feature_count_batch\n",
    "\n",
    "\n",
    "    # Exercise 5 b) ###################################################################\n",
    "    def train_batch(self, number_iterations, batch_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Implement the training procedure which uses 'batch_size' sentences from to training corpus\n",
    "        to compute the gradient.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    batch_size: int; number of sentences to use in each iteration\n",
    "                    learning_rate: float\n",
    "        Returns: last batch of sentences\n",
    "        \"\"\"\n",
    "\n",
    "        sentences = None\n",
    "\n",
    "        for iteration in range(number_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{number_iterations}\")\n",
    "\n",
    "            # Choose multiple random sentences from the corpus\n",
    "            sentences = random.choices(self.corpus, k=batch_size)\n",
    "\n",
    "            self.theta += learning_rate * (self.empirical_feature_count_batch(sentences) - self.expected_feature_count_batch(sentences))\n",
    "\n",
    "            # Reset the cached normalization factors\n",
    "            self.norm_factors = dict()\n",
    "\n",
    "        return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Exercise 5 c) ###################################################################\n",
    "def evaluate(corpus):\n",
    "    \"\"\"\n",
    "    Compare the training methods 'train' and 'train_batch' in terms of convergence rate\n",
    "    Parameters: corpus: list of list; a corpus returned by 'import_corpus'\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_accuracy(model, num_words):\n",
    "        # https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\n",
    "        # Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for _ in range(num_words):\n",
    "            random_sentence = random.choice(test_set)\n",
    "            random_tuple = random.choice(random_sentence)\n",
    "            idx_random_tuple = random_sentence.index(random_tuple)\n",
    "            word = random_tuple[0]\n",
    "            label = random_tuple[1]\n",
    "            if idx_random_tuple == 0:\n",
    "                prev_label = \"start\"\n",
    "            else:\n",
    "                prev_label = random_sentence[idx_random_tuple - 1][1]\n",
    "            pred_label = model.predict(word, prev_label)\n",
    "            if pred_label == label:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        return correct_predictions / num_words\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "\n",
    "    # Use 10% of the corpus as test set and the remaining data as training set\n",
    "    test_set = corpus[0:int(len(corpus) / 10)]\n",
    "    training_set = corpus[int(len(corpus) / 10):]\n",
    "\n",
    "    # Initialize two instances of MaxEntModel\n",
    "    model_a = MaxEntModel()\n",
    "    model_a.initialize(training_set)\n",
    "    model_b = MaxEntModel()\n",
    "    model_b.initialize(training_set)\n",
    "\n",
    "    # Initialize the word counters\n",
    "    w_a = 0\n",
    "    w_b = 0\n",
    "\n",
    "    # Initialize the accuracies\n",
    "    accuracy_a = dict()\n",
    "    accuracy_b = dict()\n",
    "\n",
    "    # Initialize the learning rates\n",
    "    learning_rate_a = 0.1\n",
    "    learning_rate_b = 0.1\n",
    "\n",
    "    num_iterations = 25\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Evaluation iteration: {iteration + 1}/{num_iterations}\")\n",
    "        # Train model A on a single random word using train()\n",
    "        model_a.train(number_iterations=1, learning_rate=learning_rate_a)\n",
    "        w_a += 1\n",
    "        accuracy_a.update({w_a: calculate_accuracy(model_a, w_a)})\n",
    "\n",
    "        # Train model B on a single random sentence using train_batch()\n",
    "        sentences = model_b.train_batch(number_iterations=1, batch_size=1, learning_rate=learning_rate_b)\n",
    "        w_b += len(sentences[0])\n",
    "        accuracy_b.update({w_b: calculate_accuracy(model_b, w_b)})\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.plot(accuracy_a.keys(), accuracy_a.values(), color='blue', label=\"Model A (train)\")\n",
    "    plt.plot(accuracy_b.keys(), accuracy_b.values(), color='red', label=\"Model B (train_batch)\")\n",
    "    ax.set_title('Accuracy vs trained number of words')\n",
    "    ax.set_xlabel('Number of trained words')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation iteration: 1/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 2/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 3/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 4/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 5/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 6/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 7/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 8/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 9/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 10/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 11/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 12/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 13/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 14/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 15/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 16/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 17/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 18/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 19/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 20/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 21/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 22/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 23/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 24/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n",
      "Evaluation iteration: 25/25\n",
      "Iteration 1/1\n",
      "Iteration 1/1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(import_corpus(\"corpus_pos.txt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_model = MaxEntModel()\n",
    "test_model.initialize(import_corpus(\"corpus_pos.txt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/5\n",
      "Iteration 2/5\n",
      "Iteration 3/5\n",
      "Iteration 4/5\n",
      "Iteration 5/5\n"
     ]
    },
    {
     "data": {
      "text/plain": "'IN'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.train_batch(3, 5)\n",
    "test_model.predict(\"Mr.\", \"CC\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}