{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Statistical Natural Language Processing (WS 20/21)\n",
    "## Exercise Sheet 3 - Manuel Hettich"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function can be used for importing the corpus.\n",
    "Parameters: path_to_file: string; path to the file containing the corpus\n",
    "Returns: list of list; the first layer list contains the sentences of the corpus;\n",
    "    the second layer list contains tuples (token,label) representing a labelled sentence\n",
    "'''\n",
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    f = open(path_to_file)\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "\n",
    "        parts = line.split(' ')\n",
    "        sentence.append((parts[0], parts[-1]))\n",
    "\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "from bidict import bidict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class MaxEntModel(object):\n",
    "    # training corpus\n",
    "    corpus = None\n",
    "\n",
    "    # (numpy) array containing the parameters of the model\n",
    "    # has to be initialized by the method 'initialize'\n",
    "    theta = None\n",
    "\n",
    "    # dictionary containing all possible features of a corpus and their corresponding index;\n",
    "    # has to be set by the method 'initialize'; hint: use a Python dictionary\n",
    "    feature_indices = None\n",
    "\n",
    "    # set containing a list of possible labels\n",
    "    # has to be set by the method 'initialize'\n",
    "    labels = None\n",
    "\n",
    "    # Caching normalization factors for (word, label)\n",
    "    norm_factors = None\n",
    "\n",
    "\n",
    "    # Exercise 1 a) ###################################################################\n",
    "    def initialize(self, corpus):\n",
    "        \"\"\"\n",
    "        Initialize the maximum entropy model, i.e., build the set of all features, the set of all labels\n",
    "        and create an initial array 'theta' for the parameters of the model.\n",
    "        Parameters: corpus: list of list representing the corpus, returned by the function 'import_corpus'\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # Initialise the set of all words in the corpus\n",
    "        words = set()\n",
    "        # Add a custom label for the beginning of sentences\n",
    "        self.labels = set()\n",
    "        self.labels.add(\"start\")\n",
    "\n",
    "        # Fill the two sets with all the words and labels in the corpus\n",
    "        for sentence in corpus:\n",
    "            for word, label in sentence:\n",
    "                words.add(word)\n",
    "                self.labels.add(label)\n",
    "\n",
    "        # Initialise the feature indices dictionary\n",
    "        self.feature_indices = bidict()\n",
    "        # Build the necessary cartesian products and store them enumerated in the dict\n",
    "        product = set()\n",
    "        product.update(itertools.product(self.labels, self.labels))\n",
    "        product.update(itertools.product(words, self.labels))\n",
    "        for idx, feature in enumerate(product):\n",
    "            self.feature_indices.update({feature: idx})\n",
    "\n",
    "        # Initialize theta\n",
    "        self.theta = np.ones(len(self.feature_indices))\n",
    "\n",
    "        # Initialize norm_factors\n",
    "        self.norm_factors = dict()\n",
    "\n",
    "    # Exercise 1 b) ###################################################################\n",
    "    def get_active_features(self, word, label, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the vector of active features.\n",
    "        Parameters: word: string; a word at some position i of a given sentence\n",
    "                    label: string; a label assigned to the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing only zeros and ones.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the indices for the two features\n",
    "        idx_word_label = self.feature_indices.get((word, label), -1)\n",
    "        idx_label_label = self.feature_indices.get((prev_label, label), -1)\n",
    "\n",
    "        # Create a numpy array with the two features set to 1\n",
    "        # according to the indices in self.feature_indices\n",
    "        active_features = np.zeros(len(self.feature_indices), dtype=bool)\n",
    "        if idx_word_label != -1:\n",
    "            active_features[idx_word_label] = True\n",
    "        if idx_label_label != -1:\n",
    "            active_features[idx_label_label] = True\n",
    "\n",
    "        return active_features\n",
    "\n",
    "    # Exercise 2 a) ###################################################################\n",
    "    def cond_normalization_factor(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the normalization factor 1/Z(x_i).\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        \"\"\"\n",
    "\n",
    "        if (word, prev_label) in self.norm_factors:\n",
    "            return self.norm_factors[(word, prev_label)]\n",
    "        else:\n",
    "            z = 0.0\n",
    "            for label in self.labels:\n",
    "                z += np.exp(np.dot(self.theta, self.get_active_features(word, label, prev_label)))\n",
    "\n",
    "            norm_factor = 1 / z\n",
    "            # Cache the normalization factor\n",
    "            self.norm_factors[(word, prev_label)] = norm_factor\n",
    "\n",
    "            return norm_factor\n",
    "\n",
    "\n",
    "    # Exercise 2 b) ###################################################################\n",
    "    def conditional_probability(self, label, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the conditional probability of a label given a word x_i.\n",
    "        Parameters: label: string; we are interested in the conditional probability of this label\n",
    "                    word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        \"\"\"\n",
    "\n",
    "        normalization_factor = self.cond_normalization_factor(word, prev_label)\n",
    "\n",
    "        return normalization_factor * np.exp(np.dot(self.theta, self.get_active_features(word, label, prev_label)))\n",
    "\n",
    "\n",
    "    # Exercise 3 a) ###################################################################\n",
    "    def empirical_feature_count(self, word, label, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the empirical feature count given a word, the actual label of this word and the label of the previous word.\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    label: string; the actual label of the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        \"\"\"\n",
    "\n",
    "        return self.get_active_features(word, label, prev_label)\n",
    "\n",
    "\n",
    "    # Exercise 3 b) ###################################################################\n",
    "    def expected_feature_count(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Compute the expected feature count given a word, the label of the previous word and the parameters of the current model\n",
    "        (see variable theta)\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        \"\"\"\n",
    "\n",
    "        expected_feature_count = np.zeros(len(self.theta))\n",
    "\n",
    "        for label in self.labels:\n",
    "            expected_feature_count += self.get_active_features(word, label, prev_label) * self.conditional_probability(label, word, prev_label)\n",
    "\n",
    "        return expected_feature_count\n",
    "\n",
    "\n",
    "    # Exercise 4 a) ###################################################################\n",
    "    def parameter_update(self, word, label, prev_label, learning_rate):\n",
    "        \"\"\"\n",
    "        Do one learning step.\n",
    "        Parameters: word: string; a randomly selected word x_i at some position i of a given sentence\n",
    "                    label: string; the actual label of the selected word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "                    learning_rate: float\n",
    "        \"\"\"\n",
    "\n",
    "        self.theta += learning_rate * (self.empirical_feature_count(word, label, prev_label) - self.expected_feature_count(word, prev_label))\n",
    "\n",
    "        # Reset the cached normalization factors\n",
    "        self.norm_factors = dict()\n",
    "\n",
    "\n",
    "    # Exercise 4 b) ###################################################################\n",
    "    def train(self, number_iterations, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Implement the training procedure.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    learning_rate: float\n",
    "        \"\"\"\n",
    "\n",
    "        for iteration in range(number_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{number_iterations}\")\n",
    "\n",
    "            # Select a random sentence from the corpus\n",
    "            random_sentence = random.choice(self.corpus)\n",
    "            # Select a random tuple from the random sentence\n",
    "            random_tuple = random.choice(random_sentence)\n",
    "            word = random_tuple[0]\n",
    "            label = random_tuple[1]\n",
    "\n",
    "            # Get the label of the previous tuple in the random sentence\n",
    "            idx_random_tuple = random_sentence.index(random_tuple)\n",
    "            if idx_random_tuple == 0:\n",
    "                prev_label = \"start\"\n",
    "            else:\n",
    "                prev_label = random_sentence[idx_random_tuple - 1][1]\n",
    "\n",
    "            self.parameter_update(word, label, prev_label, learning_rate)\n",
    "\n",
    "\n",
    "    # Exercise 4 c) ###################################################################\n",
    "    def predict(self, word, prev_label):\n",
    "        \"\"\"\n",
    "        Predict the most probable label of the word referenced by 'word'\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: string; most probable label\n",
    "        \"\"\"\n",
    "\n",
    "        maximum_probability = -np.inf\n",
    "        result_label = None\n",
    "\n",
    "        # Find the label with the maximum probability\n",
    "        for label in self.labels:\n",
    "            probability = self.conditional_probability(label, word, prev_label)\n",
    "\n",
    "            if probability > maximum_probability:\n",
    "                maximum_probability = probability\n",
    "                result_label = label\n",
    "\n",
    "        return result_label\n",
    "\n",
    "\n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def empirical_feature_count_batch(self, sentences):\n",
    "        \"\"\"\n",
    "        Predict the empirical feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returned by 'import_corpus'\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the empirical feature count array\n",
    "        empirical_feature_count_batch = np.zeros(len(self.theta), dtype=bool)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            first_word = sentence[0][0]\n",
    "            first_label = sentence[0][1]\n",
    "            empirical_feature_count_batch += self.empirical_feature_count(first_word, first_label, \"start\")\n",
    "            for idx, (word, label) in enumerate(sentence[1:]):\n",
    "                prev_label = sentence[idx - 1][1]\n",
    "                empirical_feature_count_batch += self.empirical_feature_count(word, label, prev_label)\n",
    "\n",
    "        return empirical_feature_count_batch\n",
    "\n",
    "\n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def expected_feature_count_batch(self, sentences):\n",
    "        \"\"\"\n",
    "        Predict the expected feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returned by 'import_corpus'\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the expected feature count array\n",
    "        expected_feature_count_batch = np.zeros(len(self.theta))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            first_word = sentence[0][0]\n",
    "            first_label = sentence[0][1]\n",
    "            expected_feature_count_batch += self.expected_feature_count(first_word, first_label)\n",
    "            for idx, (word, label) in enumerate(sentence[1:]):\n",
    "                prev_label = sentence[idx - 1][1]\n",
    "                expected_feature_count_batch += self.expected_feature_count(word, prev_label)\n",
    "\n",
    "        return expected_feature_count_batch\n",
    "\n",
    "\n",
    "    # Exercise 5 b) ###################################################################\n",
    "    def train_batch(self, number_iterations, batch_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Implement the training procedure which uses 'batch_size' sentences from to training corpus\n",
    "        to compute the gradient.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    batch_size: int; number of sentences to use in each iteration\n",
    "                    learning_rate: float\n",
    "        Returns: last batch of sentences\n",
    "        \"\"\"\n",
    "\n",
    "        sentences = None\n",
    "\n",
    "        for iteration in range(number_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{number_iterations}\")\n",
    "\n",
    "            # Choose multiple random sentences from the corpus\n",
    "            sentences = random.choices(self.corpus, k=batch_size)\n",
    "\n",
    "            self.theta += learning_rate * (self.empirical_feature_count_batch(sentences) - self.expected_feature_count_batch(sentences))\n",
    "\n",
    "            # Reset the cached normalization factors\n",
    "            self.norm_factors = dict()\n",
    "\n",
    "        return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Exercise 5 c) ###################################################################\n",
    "def evaluate(corpus):\n",
    "    \"\"\"\n",
    "    Compare the training methods 'train' and 'train_batch' in terms of convergence rate\n",
    "    Parameters: corpus: list of list; a corpus returned by 'import_corpus'\n",
    "    \"\"\"\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "\n",
    "    # Use 10% of the corpus as test set and the remaining data as training set\n",
    "    test_set = corpus[0:len(corpus) / 10]\n",
    "    training_set = corpus[len(corpus) / 10:]\n",
    "\n",
    "    # Initialize two instances of MaxEntModel\n",
    "    model_a = MaxEntModel()\n",
    "    model_a.initialize(training_set)\n",
    "    model_b = MaxEntModel()\n",
    "    model_b.initialize(training_set)\n",
    "\n",
    "    # Initialize the word counters\n",
    "    w_a = 0\n",
    "    w_b = 0\n",
    "\n",
    "    # Initialize the accuracies\n",
    "    accuracy_a = dict()\n",
    "    accuracy_b = dict()\n",
    "\n",
    "    # Initialize the learning rates\n",
    "    learning_rate_a = 0.1\n",
    "    learning_rate_b = 0.1\n",
    "\n",
    "    for _ in range(10):\n",
    "        model_a.train(number_iterations=1, learning_rate=learning_rate_a)\n",
    "        w_a += 1\n",
    "\n",
    "        sentences = model_b.train_batch(number_iterations=1, batch_size=1, learning_rate=learning_rate_b)\n",
    "        w_b += len(sentences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "model = MaxEntModel()\n",
    "model.initialize(import_corpus(\"corpus_pos.txt\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/5\n",
      "Iteration 2/5\n",
      "Iteration 3/5\n",
      "Iteration 4/5\n",
      "Iteration 5/5\n"
     ]
    },
    {
     "data": {
      "text/plain": "'NNP'"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_batch(5, 5)\n",
    "model.predict(\"Mr.\", \"CC\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}